{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a network for object detection\n",
    "\n",
    "This puts everything together.\n",
    "Please note: I'm not training from scratch.\n",
    "YOLO is pretrained on imagenet, I simply don't have the resources for that.\n",
    "\n",
    "So I've used this code: https://github.com/allanzelener/YAD2K\n",
    "To export tiny yolo in keras format.\n",
    "\n",
    "From the exported model, I'm cutting off the \"regression head\".\n",
    "Only the convolutions up to the last maxpooling are taken, everything after that is new and trained with my setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import cv2\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,\"/home/lars/libraries/keras/\")\n",
    "import keras\n",
    "assert keras.__version__[0] == \"2\", \"we work on version 2 of keras\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input\n",
    "from keras.layers import BatchNormalization, SpatialDropout2D\n",
    "from keras.layers.pooling import MaxPool2D\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.models import Model, load_model\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "from keras.callbacks import Callback, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### allow dynamic memory allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_model = load_model(\"models/tiny_yolo_voc.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the pretrained weights shouldn't be updated any more\n",
    "# I'm only using them for feature extraction\n",
    "for layer in extraction_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting features from the pretrained model\n",
    "Now I'm taking features from the pretrained model and concatenating them to one big feature map.\n",
    "This code is meant as a template to take features from any extraction model and merge them.\n",
    "As you can see, I'm only using the features after the last pooling.\n",
    "But you could take this code and combine it with vgg16 or something else, then you might want to take intermediate feature maps, too.\n",
    "\n",
    "The big yolo network has skip connections and does use something like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.merge import Concatenate\n",
    "\n",
    "block3 = extraction_model.get_layer(name=\"max_pooling2d_4\").output\n",
    "block4 = extraction_model.get_layer(name=\"max_pooling2d_5\").output\n",
    "block5 = extraction_model.get_layer(name=\"max_pooling2d_6\").output\n",
    "\n",
    "block3_resized = MaxPool2D((2,2), name=\"e_maxpool1\")(block3)\n",
    "block4_resized = block4\n",
    "block5_resized = block5\n",
    "\n",
    "shape3 = [int(dim) for dim in block3_resized.shape[1:3]]\n",
    "shape4 = [int(dim) for dim in block4_resized.shape[1:3]]\n",
    "shape5 = [int(dim) for dim in block5_resized.shape[1:3]]\n",
    "\n",
    "assert shape3 == shape4, \"resolution must be identical\"\n",
    "assert shape4 == shape5, \"resolution must be identical\"\n",
    "\n",
    "#extracted_features = Concatenate(axis=-1)([\n",
    "#    block3_resized, \n",
    "#    block4_resized, \n",
    "#    block5_resized])\n",
    "extracted_features = block5_resized\n",
    "extracted_features.shape.as_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A new model\n",
    "This recreates the layout of tiny yolo.\n",
    "But the layers are not trained yet. This way I can check if the setup really works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "B = 5   # number of anchor boxes\n",
    "C = 20  # number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# start with the extracted features\n",
    "conv = extracted_features\n",
    "\n",
    "# block 1\n",
    "conv = Conv2D(1024, 3, \n",
    "              padding=\"same\",\n",
    "              use_bias = False,\n",
    "              kernel_regularizer = keras.regularizers.l2(0.0005),\n",
    "              name=\"head_conv1\")(conv)\n",
    "conv = BatchNormalization(name=\"head_bnorm1\")(conv)\n",
    "conv = LeakyReLU(0.1, name=\"head_lrelu1\")(conv)\n",
    "conv = SpatialDropout2D(0.3)(conv)\n",
    "\n",
    "# block 2\n",
    "conv = Conv2D(1024, 3, \n",
    "              padding=\"same\",\n",
    "              use_bias = False,\n",
    "              kernel_regularizer = keras.regularizers.l2(0.0005),\n",
    "              name=\"head_conv2\")(conv)\n",
    "conv = BatchNormalization(name=\"head_bnorm2\")(conv)\n",
    "conv = LeakyReLU(0.1, name=\"head_lrelu2\")(conv)\n",
    "conv = SpatialDropout2D(0.15)(conv)\n",
    "\n",
    "# output\n",
    "conv = Conv2D(B *(C+5), 1, \n",
    "              padding=\"same\",\n",
    "              use_bias = True,\n",
    "              kernel_regularizer = keras.regularizers.l2(0.0005),\n",
    "              name=\"head_conv3\")(conv)\n",
    "\n",
    "detection_model = Model(inputs=extraction_model.input, outputs = conv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parameters of the model\n",
    "\n",
    "These are training parameters.\n",
    "The input and output resolution are important for setting up the boxes as loss for training.\n",
    "The lambdas are factors to weigh the different loss components against each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_tensor = detection_model.input\n",
    "\n",
    "in_x = int(input_tensor.shape[1])\n",
    "in_y = int(input_tensor.shape[2])\n",
    "\n",
    "output_tensor = detection_model.output\n",
    "\n",
    "out_x = int(output_tensor.shape[1])\n",
    "out_y = int(output_tensor.shape[2])\n",
    "\n",
    "lambda_coords = 10\n",
    "lambda_class = 2\n",
    "lambda_obj = 5\n",
    "lambda_noobj = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Set up the training data\n",
    "Follow the guide on the darknet side to set up VOC:\n",
    "https://pjreddie.com/darknet/yolo/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare a config for the augmentations\n",
    "config={}\n",
    "config[\"max_hsv_scale\"] = [0.1, 0.5, 0.5]\n",
    "config[\"max_rotation\"] = 10\n",
    "config[\"max_shift\"] = 0.05\n",
    "config[\"zoom_range\"] = (0.8,1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_path= \"/home/lars/data/darknet/VOC/train.txt\"\n",
    "test_path = \"/home/lars/data/darknet/VOC/2007_test.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterator class to provide data to model.fit_generator\n",
    "from generator import Augmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "\n",
    "# generators for training data and test data\n",
    "train_gen = Augmenter(train_path, \n",
    "                      in_x, in_y, out_x, out_y,\n",
    "                      B, C, batch_size = batch_size)\n",
    "val_gen = Augmenter(test_path, \n",
    "                      in_x, in_y, out_x, out_y,\n",
    "                      B, C, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the generator\n",
    "batch = next(val_gen)\n",
    "imgs = batch[0]\n",
    "objects = batch[1]\n",
    "    \n",
    "plt.imshow(imgs[0, :,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function\n",
    "\n",
    "The loss function makes use of currying. Therefore this code is a little complicated.\n",
    "Keras expects a loss in this format loss(y_true, y_pred).\n",
    "\n",
    "But the loss_func in loss.py needs to know additional parameters such as the network size.\n",
    "I'm feeding that data by currying the loss_func and providing the additional parameters now.\n",
    "The result is a function with two remaining parameters and a signature as expected by keras.\n",
    "\n",
    "This currying can go very wrong, if you mix up the order of the parameters.\n",
    "If the loss function is called, it prints the parameters it has been given.\n",
    "Be sure to check this.\n",
    "Look at model.compile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anchor boxes are taken from the tiny yolo voc config\n",
    "anchors = np.zeros((B, 2))\n",
    "anchors[:] =[[1.08,1.19],  [3.42,4.41],  [6.63,11.38],  [9.42,5.11],  [16.62,10.52]]\n",
    "\n",
    "# the anchors are given as width, height\n",
    "# this doesn't work with numpy's layout\n",
    "# we have to switch the x and y dimensions\n",
    "temp = anchors[:,0].copy()\n",
    "anchors[:,0]=anchors[:,1]\n",
    "anchors[:,1]= temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loss_function import loss_func\n",
    "\n",
    "\n",
    "meta_data = [anchors, out_x, out_y, B, C, lambda_class, lambda_coords, lambda_obj, lambda_noobj]\n",
    "loss = loss_func(*meta_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Training the model\n",
    "Compile with the custom loss, set up a few callbacks and train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam, SGD\n",
    "\n",
    "# check this: are the parameters correct ?\n",
    "detection_model.compile(Adam(lr=0.0001), loss)\n",
    "\n",
    "#detection_model.compile(SGD(lr=1e-4, momentum=0.9, decay = 1e-7), loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# taken from the keras source\n",
    "# if the learning rate is too fast, NaNs can occur, stop the training in this case\n",
    "class TerminateOnNaN(Callback):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.seen = 0\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        self.seen += 1\n",
    "        \n",
    "        logs = logs or {}\n",
    "        loss = logs.get('loss')\n",
    "        \n",
    "        if loss is not None:\n",
    "            if np.isnan(loss) or np.isinf(loss):\n",
    "                print('Batch %d: Invalid loss, terminating training' % (batch))\n",
    "                print(\"logs: \", logs)\n",
    "                \n",
    "                self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_terminator = TerminateOnNaN()\n",
    "\n",
    "# train in small steps and append histories\n",
    "# if training is interrupted, the history array still contains usable data\n",
    "import time\n",
    "histories = []\n",
    "times = []\n",
    "for i in range(20):\n",
    "\n",
    "    history=detection_model.fit_generator(train_gen, 6400//batch_size, \n",
    "                                          epochs=10, \n",
    "                                          callbacks=[nan_terminator],\n",
    "                                          validation_data = val_gen,\n",
    "                                          validation_steps = 1600//batch_size,\n",
    "                                          #use_multiprocessing=False)\n",
    "                                          workers =4,\n",
    "                                          max_queue_size=24)\n",
    "    histories.append(history)\n",
    "    times.append(time.time())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the test / val loss\n",
    "As you can see, the model reaches about 1000 for validation loss.\n",
    "Then it overfits.\n",
    "\n",
    "This number can't be interpreted correctly. It depends on the size of the network and the batch.\n",
    "A solution would be to take the mean in the loss instead of summing all components.\n",
    "But that would mess with the learning rate.\n",
    "\n",
    "I'm evaluating the pretrained model against the validation generator.\n",
    "Surprisingly, the new model reaches better scores.\n",
    "A possible explanation: The original yolo doesn't use rotations as augmentations. The validation generator uses rotations.\n",
    "Or the number of samples from the validation set was simply too small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "val_losses = []\n",
    "\n",
    "for item in histories:\n",
    "    losses.extend(item.history[\"loss\"])\n",
    "    val_losses.extend(item.history[\"val_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(losses)\n",
    "plt.plot(val_losses)\n",
    "plt.legend([\"train\", \"val\"])\n",
    "plt.title(\"training loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_model.compile(Adam(lr=0.0001), loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(histories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_model.evaluate_generator(val_gen, 12800//batch_size, max_queue_size=20, workers=4, use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two important helper functions to work with the data\n",
    "with get_probabilities you can extract the predicted classes, objectness and the combined probability from the output of the network\n",
    "\n",
    "the extract_from_blob helps with the blob of data fed to the keras loss.\n",
    "This blob is hard to read, so the function slices the individual parts out of it and converts them to a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from prediction_utils import extract_from_blob, get_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gen = val_gen\n",
    "# get some sample data\n",
    "batch = next(test_gen)\n",
    "img = batch[0].copy()\n",
    "\n",
    "plt.imshow(img[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feed the data to the model\n",
    "preds = detection_model.predict(batch[0])\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing given objectness with confidence of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the given objectness for this image\n",
    "loss_dict = extract_from_blob(batch[1], out_x, out_y, B, C)\n",
    "\n",
    "# read the given objectness out of the loss dictionary\n",
    "f_objectness = loss_dict[\"f_objectness\"].reshape((-1,out_x, out_y, B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data out of the predictions\n",
    "classes, objectness, probs = get_probabilities(preds)\n",
    "\n",
    "# probs is along the B dimension\n",
    "# for every cell in the output activation map, get the best bounding box score\n",
    "max_probs = probs.max(axis=-1)\n",
    "\n",
    "threshold = 0.3\n",
    "thresholded = max_probs > threshold\n",
    "\n",
    "f, axes = plt.subplots(1, 3, figsize=(10,10))\n",
    "\n",
    "\n",
    "axes[0].imshow(f_objectness[0,:,:,0])\n",
    "axes[0].set_title(\"given objectness\")\n",
    "\n",
    "axes[1].imshow(max_probs)\n",
    "axes[1].set_title(\"confidence\")\n",
    "\n",
    "axes[2].imshow(thresholded)\n",
    "axes[2].set_title(\"thresholded\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the predicted bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# which coordinates are bigger than the threshold ?\n",
    "xy = np.where(thresholded)\n",
    "\n",
    "# reshape the predictions to expose the anchor boxes along the 4th axis\n",
    "preds = preds.reshape((-1, out_x, out_y, B, 4 + 1 + C))\n",
    "\n",
    "# array to store the detections\n",
    "detections=[]\n",
    "\n",
    "# look at all the coordinates found by the thresholding\n",
    "for row, col in zip(xy[0], xy[1]):\n",
    "    \n",
    "    # for this coordinate, find the box with the highest objectness\n",
    "    current_probs = objectness[row, col]\n",
    "    box_idx = np.argmax(current_probs)\n",
    "    box = preds[0, row, col, box_idx]\n",
    "    \n",
    "    # get the predicted coordinates, convert them to percent\n",
    "    # this is the same code as in the generator and the loss function\n",
    "    # the network learns to predict coordinates encoded in this way\n",
    "    p_x = (row + np_sigmoid(box[0])) / out_x\n",
    "    p_y = (col + np_sigmoid(box[1])) / out_y\n",
    "    p_dx = (np.exp(box[2]) * anchors[box_idx, 0]) / out_x\n",
    "    p_dy = (np.exp(box[3]) * anchors[box_idx, 1]) / out_y\n",
    "    \n",
    "    # resize the predicted coordinates to the input resolution\n",
    "    min_x = int ((p_x - p_dx/2.) * in_x)\n",
    "    max_x = int ((p_x + p_dx/2.) * in_x)\n",
    "    min_y = int ((p_y - p_dy/2.) * in_y)\n",
    "    max_y = int ((p_y + p_dy/2.) * in_y)\n",
    "    \n",
    "    # clip them to the image size\n",
    "    min_x = max(min_x, 0)\n",
    "    max_x = min(max_x, in_x)\n",
    "    min_y = max(min_y, 0)\n",
    "    max_y = min(max_y, in_y)\n",
    "    \n",
    "    # get the highest class prediction\n",
    "    current_classes = classes[row, col, box_idx]\n",
    "    label = np.argmax(current_classes)\n",
    "    \n",
    "    \n",
    "    detections.append((label, min_x, max_x, min_y, max_y, current_probs.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of detections: \", len(detections))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Max Suppression\n",
    "Sometimes yolo will predict the same object in more than one cell.\n",
    "This happens mostly for very big objects where the center of the object is not clear.\n",
    "\n",
    "We need non-max suppression to remove overlapping bounding boxes.\n",
    "\n",
    "We apply the non-max suppression to each label separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# taken from the yolo repository\n",
    "names = [\"aeroplane\",\n",
    "\"bicycle\",\n",
    "\"bird\",\n",
    "\"boat\",\n",
    "\"bottle\",\n",
    "\"bus\",\n",
    "\"car\",\n",
    "\"cat\",\n",
    "\"chair\",\n",
    "\"cow\",\n",
    "\"diningtable\",\n",
    "\"dog\",\n",
    "\"horse\",\n",
    "\"motorbike\",\n",
    "\"person\",\n",
    "\"pottedplant\",\n",
    "\"sheep\",\n",
    "\"sofa\",\n",
    "\"train\",\n",
    "\"tvmonitor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sort the detections, create a dictionary that maps labels to detections and their confidence scores\n",
    "label_dict={}\n",
    "\n",
    "for detection in detections:\n",
    "    \n",
    "    label, min_x, max_x, min_y, max_y, score = detection\n",
    "    \n",
    "    if label in label_dict:\n",
    "        label_dict[label].append(((min_x, min_y, max_x, max_y), score))\n",
    "    else:\n",
    "        label_dict[label] = [((min_x, min_y, max_x, max_y), score)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a new dictionary. Again, it maps labels to detections\n",
    "# but the detections are now filtered with non-max suppression\n",
    "nms = {}\n",
    "\n",
    "for label in label_dict:\n",
    "    boxes = [box for (box, score) in label_dict[label]]\n",
    "    scores = [score for (box, score) in label_dict[label]]\n",
    "    \n",
    "    # tensorflow has a built-in algorithm for non-max suppresion\n",
    "    # the result is an array of indexes into the list of boxes\n",
    "    # those indices are the chosen / retained boxes\n",
    "    # unfortunately, the list is a tensor\n",
    "    # we need a session to evaluate the tensor\n",
    "    # at the very top of this notebook we have created this session\n",
    "    idx = tf.image.non_max_suppression(boxes, scores, 5, iou_threshold=0.2)\n",
    "    idx = sess.run(idx)\n",
    "    \n",
    "    # boxes we keep\n",
    "    boxes = [boxes[i] for i in idx]\n",
    "    \n",
    "    nms[label] = boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert the mapping in nms\n",
    "# instead of label -> list of boxes\n",
    "# we now have\n",
    "# name -> list of boxes\n",
    "nms = {names[key]: value for key,value in nms.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"we found the following boxes after non-max suppression\")\n",
    "print(nms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the output\n",
    "I'm using opencv to draw rectangles around all detections\n",
    "and to write the name in text onto the image.\n",
    "\n",
    "The image has a very low resolution.\n",
    "For the output it is upscaled.\n",
    "The main reason for this is to allow high-res text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img = batch[0][0]\n",
    "output_img = img.copy()\n",
    "dim_x, dim_y = output_img.shape[:2]\n",
    "factor = 5\n",
    "output_img = cv2.resize(output_img, (dim_y*factor, dim_x*factor))\n",
    "\n",
    "for label in nms:\n",
    "    boxes = nms[label]\n",
    "    for box in boxes:\n",
    "        min_x, min_y, max_x, max_y = box\n",
    "        min_x *= factor\n",
    "        min_y *= factor\n",
    "        max_x *= factor\n",
    "        max_y *= factor\n",
    "        \n",
    "        cv2.rectangle(output_img,(min_y, min_x),(max_y, max_x),(0,1,0),10)\n",
    "        cv2.putText(output_img, label, (min_y, min_x), cv2.FONT_HERSHEY_SIMPLEX, fontScale=3.5, color=(0, 0, 1), thickness=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(output_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
